{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a folder to put files for aspect extraction we will use temporary folder that was created for sentiment analysis task in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, '/home/ayaqubov/sentvec_new/italian/')\n",
    "# from fasttext import FastVector\n",
    "# en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "# import numpy as np\n",
    "# def find_min_distance(word,list_of_categories):\n",
    "#     l=len(list_of_categories)\n",
    "#     distances=[]\n",
    "#     for i in range(0,l):\n",
    "#         a=en_model[word]\n",
    "#         b=en_model[list_of_categories[i]]\n",
    "#         d=np.linalg.norm(a-b)\n",
    "#         distances.append(d)\n",
    "#     return distances.index(min(distances))\n",
    "\n",
    "# ### all these functions below make up the aspect extraction from the sentence\n",
    "\n",
    "# ## This file contains functions needed to extract the aspects\n",
    "# def word_tokens(mysentence):\n",
    "#     # tokenization\n",
    "#     import nltk\n",
    "#     from nltk.tokenize import RegexpTokenizer\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     #mysent='This camera is sleek, heavy and very affordable.' #I liked the book. Not to mention the price of the phone. '\n",
    "#     #mysent='Love the sleekness of the player.'\n",
    "#     #mysent='Very big to hold'\n",
    "#     #mysent='Not to mention the price of the phone.'\n",
    "#     #mysent='We ordered chicken casserole, but what we got were a few small pieces of chicken, all dark meat and on the bone.'\n",
    "#     #mysentence='Logitech mouse was nice.'\n",
    "#     mywords=tokenizer.tokenize(mysentence)\n",
    "#     tags=nltk.pos_tag(mywords)\n",
    "#     return tags\n",
    "\n",
    "# def do_stemming(word):\n",
    "#     from nltk.stem.lancaster import LancasterStemmer\n",
    "#     st = LancasterStemmer()\n",
    "#     stemmed=st.stem(word)\n",
    "#     return stemmed\n",
    "\n",
    "# def dependency_parse_sentence(mysentence):\n",
    "#     from nltk.parse.stanford import StanfordDependencyParser\n",
    "#     path_to_jar='/home/ayaqubov/stanford_nlp/stanford_parser/stanford-parser.jar'\n",
    "#     path_to_models_jar='/home/ayaqubov/stanford_nlp/stanford_parser/stanford-parser-3.5.2-models.jar'\n",
    "#     dependency_parser=StanfordDependencyParser(path_to_jar=path_to_jar,path_to_models_jar=path_to_models_jar)\n",
    "#     result = dependency_parser.raw_parse(mysentence)\n",
    "#     dep = result.next()\n",
    "#     dependency_results=list(dep.triples())\n",
    "#     #print \"Dependency results:::\"\n",
    "#     #print dependency_results\n",
    "#     return dependency_results\n",
    "\n",
    "\n",
    "# def extract_explicit(mysentence):\n",
    "#     ### finding explicit aspects based on the rules\n",
    "#     dependency_results=dependency_parse_sentence(mysentence)\n",
    "#     aspects_explicit=[]\n",
    "#     ld=len(dependency_results)\n",
    "#     for i in range(0,ld):\n",
    "#         #rule 1\n",
    "#         if(dependency_results[i][1] in ['amod','nmod','nummod','appos','det','nsubjpass']):# among nominal modifiers of the noun\n",
    "#             if(dependency_results[i][2][1] in ['NN','NNP']):\n",
    "#                 asp=dependency_results[i][2][0]\n",
    "#                 aspects_explicit.append(asp)\n",
    "\n",
    "#         # rule 1\n",
    "#         if(dependency_results[i][1] in ['nsubj','dobj','iobj']): #adverbial or adjective modifier ---clausal argument relations\n",
    "#             if(dependency_results[i][2][1] in ['NN','NNP'] and dependency_results[i][0][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "#                 asp=dependency_results[i][2][0]\n",
    "#                 aspects_explicit.append(asp)\n",
    "\n",
    "#         # rule 2.\n",
    "#         if(dependency_results[i][1] in ['nsubj','dobj','iobj']):\n",
    "#             if(dependency_results[i][2][1] in ['NN','NNP'] and dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ']):\n",
    "#                 asp=dependency_results[i][2][0]\n",
    "#                 aspects_explicit.append(asp)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "#     if(dependency_results[i][1]=='cop'):\n",
    "#         if(dependency_results[i][0][1] in ['NNP','NNPS']):\n",
    "#             aspects_explicit.append(dependency_results[i][0][1])\n",
    "    \n",
    "    \n",
    "#         # rule 3\n",
    "\n",
    "#     ##### sentences which do not have subject noun relation in their parse tree\n",
    "#     #rule 3.3.4 .2\n",
    "#     for ii in range(0,ld):\n",
    "#         if(dependency_results[ii][1]=='case'):\n",
    "#             mynoun=dependency_results[ii][0][0]\n",
    "#             for kk in range(0,ld):\n",
    "#                 if(dependency_results[kk][2][0]==mynoun and dependency_results[kk][0][1] in ['NN','NNS','NNP','NNPS']):\n",
    "#                     aspects_explicit.append(dependency_results[kk][0][0])\n",
    "#                     if(dependency_results[ii][0][1] !='PRP'):\n",
    "#                         aspects_explicit.append(mynoun)\n",
    "\n",
    "#     ### among additional rules:\n",
    "#     # rule 3.3.5. 2\n",
    "#     for iii in range(0,ld):\n",
    "#         if(dependency_results[iii][1]=='compound'):\n",
    "#             if(dependency_results[iii][0][1] in ['NN','NNS','NNP','NNPS'] and dependency_results[iii][2][1] in ['NN','NNS','NNP','NNPS']):\n",
    "#                 new_aspect=dependency_results[iii][2][0]+' '+dependency_results[iii][0][0]\n",
    "#                 aspects_explicit.append(new_aspect)\n",
    "#     unique_explicit_aspects=list(set(aspects_explicit))     \n",
    "#     print('Explicit aspects are :::')\n",
    "#     print(unique_explicit_aspects)\n",
    "#     return unique_explicit_aspects\n",
    "\n",
    "\n",
    "# def importance_of_aspects(aspects_explicit):\n",
    "#     #printing the importance of the explicit aspects\n",
    "#     from collections import Counter\n",
    "#     counter_aspects=Counter(aspects_explicit)\n",
    "#     print(counter_aspects)\n",
    "\n",
    "# def find_max_index(mylist):\n",
    "#         import numpy as np\n",
    "#         ind = np.argmax(mylist)\n",
    "#         return ind\n",
    "\n",
    "\n",
    "# # def extract_implicit(mysentence):\n",
    "# #     dependency_results=dependency_parse_sentence(mysentence)\n",
    "# #     print dependency_results\n",
    "# #     ## getting the implicit aspects\n",
    "\n",
    "# #     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "\n",
    "# #     implicit_aspects=[]\n",
    "# #     dl=len(dependency_results)\n",
    "# #     for i in range(0,dl):\n",
    "# #         if(dependency_results[i][1]=='cop'):## if the token is in copular relation with the copular verb and copular verb exists in implicit aspect lexicon\n",
    "# #             if(dependency_results[i][0][1] not in ['NNP','NNPS']):\n",
    "# #                 implicit_indicator=dependency_results[i][0][0]\n",
    "# #                 implicit_aspects.append(implicit_indicator)\n",
    "\n",
    "# #                 for kk in range(0,len(dependency_results)):\n",
    "# #                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][1]=='conj'):\n",
    "# #                         #implicit_indicator=dependency_results[kk][2][0]\n",
    "# #                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "\n",
    "# #                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][2][1]=='VB'):\n",
    "# #                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "                        \n",
    "                        \n",
    "# #          ## check for copular verbs\n",
    "# #         copular_verbs=['is', 'am', 'are', 'was', 'were', 'appear', 'seem', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get']\n",
    "# #         ####Use copular verbs\n",
    "# #         #if the verb is modified by adjective and adverb:\n",
    "# #         if(dependency_results[i][1] in ['nsubj','dobj','iobj','ccomp','xcomp'] ):\n",
    "# #             if(dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ'] and dependency_results[i][2][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "# #                 #if the noun is in subject relation verb\n",
    "# #                 #take that verb:\n",
    "# #                 myverb=dependency_results[i][0][0]\n",
    "# #                 # search for the noun in that sentence\n",
    "                \n",
    "# #                 stem_verb=do_stemming(myverb)\n",
    "# #                 implicit_aspects.append(stem_verb)\n",
    "\n",
    "# #         if(dependency_results[i][1]=='amod'):\n",
    "# #             if(dependency_results[i][2][1]=='JJ' and dependency_results[i][0][1] in ['NN','NNS']):\n",
    "# #                 implicit_aspects.append(dependency_results[i][2][0])\n",
    "                \n",
    "# #         if(dependency_results[i][1]=='nsubjpass'):\n",
    "# #             if(dependency_results[i][0][1] in ['JJ','JJR','JJS'] and dependency_results[i][2][1] in ['NN','NNS']):\n",
    "# #                 implicit_aspects.append(dependency_results[i][0][0])\n",
    "                \n",
    "# #     print('Implicit aspects are::')\n",
    "# #     print(implicit_aspects)\n",
    "\n",
    "# #     import gensim\n",
    "# #     from gensim.models import word2vec\n",
    "\n",
    "# #     # get the pretrained word2vec model \n",
    "# #     fname=\"/home/ayaqubov/AspectExtractor/myword2vec/word2vecmodel_electronics\"\n",
    "# #     #mymodel.save(fname)\n",
    "# #     # To get back the model\n",
    "# #     mymodel = gensim.models.Word2Vec.load(fname)\n",
    "    \n",
    "# #     # remove non-frequent implicit aspects from the sentences --->>> this is needed because we want to compare these to the categories\n",
    "# #     unique_implicit_aspects=list(set(implicit_aspects))\n",
    "# #     print 'Implicit aspects here:::'\n",
    "# #     print unique_implicit_aspects\n",
    "    \n",
    "# #     ### mapping of implicit aspect categories\n",
    "# #     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "# #     list2=[]\n",
    "\n",
    "# #     for i in range(0,len(unique_implicit_aspects)):\n",
    "# #         list2.append(unique_implicit_aspects[i])\n",
    "\n",
    "# #     similarities=[]\n",
    "# #     for word2 in list2:\n",
    "# #         s=[]\n",
    "# #         for asp_cat in implicit_aspect_categories:\n",
    "# #             try:\n",
    "# #                 s.append(mymodel.similarity(word2,asp_cat))\n",
    "# #             except:\n",
    "# #                 # word is not found in categories\n",
    "# #                 s.append(word2)\n",
    "# #         similarities.append(s)\n",
    "\n",
    "# #     max_indices=[]\n",
    "# #     for i in range(0,len(similarities)):\n",
    "# #         m=find_max_index(similarities[i])\n",
    "# #         max_indices.append(m)\n",
    "\n",
    "# #     # this part does aspect category paring using the results from the WORD2VEC\n",
    "# #     aspect_category_pair = dict()\n",
    "# #     for i in range(0,len(unique_implicit_aspects)):\n",
    "# #         max_l=max_indices[i]\n",
    "# #         aspect_category_pair[unique_implicit_aspects[i]]=implicit_aspect_categories[max_l]\n",
    "\n",
    "# #     #combining implicit and explicit aspects\n",
    "# #     #total_aspects=aspects_explicit\n",
    "# #     implicit_categories=aspect_category_pair.values()\n",
    "# #     return implicit_categories\n",
    "\n",
    "\n",
    "\n",
    "# def extract_implicit(mysentence):\n",
    "#     dependency_results=dependency_parse_sentence(mysentence)\n",
    "#     print dependency_results\n",
    "#     ## getting the implicit aspects\n",
    "\n",
    "#     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "\n",
    "#     implicit_aspects=[]\n",
    "#     dl=len(dependency_results)\n",
    "#     for i in range(0,dl):\n",
    "#         if(dependency_results[i][1]=='cop'):## if the token is in copular relation with the copular verb and copular verb exists in implicit aspect lexicon\n",
    "#             if(dependency_results[i][0][1] not in ['NNP','NNPS']):\n",
    "#                 implicit_indicator=dependency_results[i][0][0]\n",
    "#                 implicit_aspects.append(implicit_indicator)\n",
    "\n",
    "#                 for kk in range(0,len(dependency_results)):\n",
    "#                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][1]=='conj'):\n",
    "#                         #implicit_indicator=dependency_results[kk][2][0]\n",
    "#                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "\n",
    "#                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][2][1]=='VB'):\n",
    "#                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "                        \n",
    "                        \n",
    "#          ## check for copular verbs\n",
    "#         copular_verbs=['is', 'am', 'are', 'was', 'were', 'appear', 'seem', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get']\n",
    "#         ####Use copular verbs\n",
    "#         #if the verb is modified by adjective and adverb:\n",
    "#         if(dependency_results[i][1] in ['nsubj','dobj','iobj','ccomp','xcomp'] ):\n",
    "#             if(dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ'] and dependency_results[i][2][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "#                 #if the noun is in subject relation verb\n",
    "#                 #take that verb:\n",
    "#                 myverb=dependency_results[i][0][0]\n",
    "#                 # search for the noun in that sentence\n",
    "                \n",
    "#                 stem_verb=do_stemming(myverb)\n",
    "#                 implicit_aspects.append(stem_verb)\n",
    "\n",
    "#         if(dependency_results[i][1]=='amod'):\n",
    "#             if(dependency_results[i][2][1]=='JJ' and dependency_results[i][0][1] in ['NN','NNS']):\n",
    "#                 implicit_aspects.append(dependency_results[i][2][0])\n",
    "                \n",
    "#         if(dependency_results[i][1]=='nsubjpass'):\n",
    "#             if(dependency_results[i][0][1] in ['JJ','JJR','JJS'] and dependency_results[i][2][1] in ['NN','NNS']):\n",
    "#                 implicit_aspects.append(dependency_results[i][0][0])\n",
    "\n",
    "#     import gensim\n",
    "#     from gensim.models import word2vec\n",
    "    \n",
    "#     # get the pretrained word2vec model \n",
    "#     #fname=\"/home/ayaqubov/AspectExtractor/myword2vec/word2vecmodel_electronics\"\n",
    "#     #mymodel.save(fname)\n",
    "#     # To get back the model\n",
    "#     #mymodel = gensim.models.Word2Vec.load(fname)\n",
    "    \n",
    "#     #en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "#     #mymodel\n",
    "#     #en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "    \n",
    "\n",
    "#     # remove non-frequent implicit aspects from the sentences --->>> this is needed because we want to compare these to the categories\n",
    "#     unique_implicit_aspects=list(set(implicit_aspects))\n",
    "#     print 'Implicit aspects here:::'\n",
    "#     print unique_implicit_aspects\n",
    "    \n",
    "#     ### mapping of implicit aspect categories\n",
    "#     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "#     list2=[]\n",
    "    \n",
    "    \n",
    "#     output_categories=[]\n",
    "#     for k in range(0,len(unique_implicit_aspects)):\n",
    "#         asp_=unique_implicit_aspects[k]\n",
    "        \n",
    "#         try:\n",
    "#             a=find_min_distance(asp_,implicit_aspect_categories)\n",
    "#             output_categories.append(implicit_aspect_categories[a])\n",
    "#         except:\n",
    "#             print \"Implicit aspect directly goes to output ... Model does not contain the word.\"\n",
    "#             output_categories.append(asp_)\n",
    "            \n",
    "        \n",
    "    \n",
    "#     return output_categories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def combine_aspects(aspects_explicit,implicit_categories):\n",
    "#     total_aspects=aspects_explicit\n",
    "    \n",
    "#     for ij in range(0,len(implicit_categories)):\n",
    "#         total_aspects.append(implicit_categories[ij])\n",
    "#     #here are the total aspects\n",
    "#     # these aspects represnet the sentence\n",
    "#     #print(total_aspects)\n",
    "#     #now issue is to find the overall \n",
    "#     return total_aspects\n",
    "#     total_aspects=aspects_explicit\n",
    "    \n",
    "#     return total_aspects\n",
    "\n",
    "\n",
    "\n",
    "# #####################\n",
    "# #2 ways to run : either run the main function or run the get_aspects function\n",
    "\n",
    "# ########################\n",
    "\n",
    "# def get_aspects(mysentence):\n",
    "#     import os\n",
    "#     from nltk.parse import stanford\n",
    "#     import os\n",
    "#     java_path = \"/usr/lib/jvm/jre-1.8.0\"\n",
    "#     os.environ['JAVAHOME'] = java_path\n",
    "#     myimplicit=extract_implicit(mysentence)\n",
    "#     myexplicit=extract_explicit(mysentence)\n",
    "#     total_aspects=combine_aspects(myexplicit,myimplicit)\n",
    "#     if(total_aspects==[]): # when we dont have specific aspect in the sentence, we add 'GENERAL'\n",
    "#         total_aspects.append('GENERAL')\n",
    "        \n",
    "#     stotal_aspects=set(total_aspects)\n",
    "#     return stotal_aspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'black', u'JJ'), u'nsubj', (u'interface', u'NN')), ((u'interface', u'NN'), u'det', (u'This', u'DT')), ((u'black', u'JJ'), u'cop', (u'is', u'VBZ')), ((u'black', u'JJ'), u'cc', (u'but', u'CC')), ((u'black', u'JJ'), u'conj', (u'liked', u'VBD')), ((u'liked', u'VBD'), u'nsubj', (u'i', u'FW')), ((u'liked', u'VBD'), u'dobj', (u'background', u'NN'))]\n",
      "Implicit aspects are::\n",
      "[u'black', u'liked']\n",
      "Implicit aspects here:::\n",
      "[u'liked', u'black']\n",
      "Explicit aspects are :::\n",
      "[u'interface', u'background']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'appearance', u'background', u'interface'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_aspects('This interface is black, but i liked background.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # this is the code needed for running the sentiment analysis engine\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from langdetect import detect\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# import nltk\n",
    "# #from textblob import TextBlob,\n",
    "# from textblob import Word, Blobber\n",
    "# import textblob_fr\n",
    "# import textblob_de\n",
    "# import os\n",
    "# import shutil # from removing files from one folder to another\n",
    "# os.environ[\"NLS_LANG\"] = \".AL32UTF8\" \n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "\n",
    "# ############\n",
    "# ## the directories shown below should be changed when the new folder for the aspect extraction is given\n",
    "# ############\n",
    "\n",
    "# import os\n",
    "# input_directory='/home/ayaqubov/engines/engines/Windows-share/Input/'\n",
    "# files_in_folder=os.listdir(input_directory)\n",
    "# output_directory='/home/ayaqubov/engines/Windows-share/Output/'\n",
    "# processed_directory='/home/ayaqubov/engines/engines/Windows-share/Processed/'\n",
    "\n",
    "\n",
    "# # check while the input is empty do the \n",
    "\n",
    "# #while(os.listdir('/home/ayaqubov/engines/Windows-share/Input/')!=\"\"):\n",
    "# # get the data and process the data\n",
    "\n",
    "# for ifiles in range(0,len(files_in_folder)):\n",
    "#     file_directory=input_directory + files_in_folder[ifiles]\n",
    "#     print(file_directory)\n",
    "#     #file_directory\n",
    "#     mycolumnsdf=['REVIEW_WID','REVIEW_FULL_TEXT']\n",
    "#     df_aws_reviews=pd.DataFrame(columns=mycolumnsdf)\n",
    "#     df_aws_reviews=pd.read_csv(file_directory)\n",
    "#     df_aws_reviews['LANGUAGE']=\"\"\n",
    "#     num_unique_reviews=df_aws_reviews.shape[0]\n",
    "\n",
    "#     ###########################################################\n",
    "#     # here comes the check the language part and write correspondingly to the df_aws_reviews dataframe\n",
    "\n",
    "#     from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#     ## data processing\n",
    "#     print('Processing...')\n",
    "#     index=0\n",
    "    \n",
    "#     for u in range(0,num_unique_reviews):\n",
    "#         review_id=df_aws_reviews.iloc[u,0]\n",
    "#         print(review_id)\n",
    "#         review_text=df_aws_reviews.iloc[u,1]\n",
    "#         review_str=str(review_text)\n",
    "#         #review_strw=review_str.encode('cp1252')## windows encoded\n",
    "#         #ureview_str=review_strw.decode('utf-8')\n",
    "#         #ureview_str=unicode(review_str,\"utf-8\")\n",
    "#         ureview_str=review_str.decode('cp1252').encode('utf-8',errors='ignore')\n",
    "#         #print(review_str)\n",
    "#         #print(len(review_str))\n",
    "#         contain_l=re.search('[a-zA-Z]', review_str)\n",
    "#         if(contain_l!='None'):\n",
    "#             # handle japanese,arabic,chinese cases because they appear as the ?? marks in the results\n",
    "#             try:\n",
    "#                 text_lang=detect(ureview_str)\n",
    "#                 #text_lang2=identify_lang(review_str)\n",
    "#             except:\n",
    "#                 print 'Error in reading, keep reading'\n",
    "#                 #i_debug+=1\n",
    "#                 continue\n",
    "#             #check_words=word_tokenize(review_str)\n",
    "#             #for iiii in range(0,len(check_the_words)):\n",
    "#             #    if (check_the_words[iiii]=='product' or check_the_words[iiii]=='excellent'  or check_the_words[iiii]=='mouse'):\n",
    "#             #        break\n",
    "#             #            #continue\n",
    "\n",
    "#             if(text_lang=='en'):\n",
    "#                 df_aws_reviews.iloc[u,2]='English'\n",
    "#                 index+=1\n",
    "\n",
    "#             if(text_lang=='de'):\n",
    "#                 df_aws_reviews.iloc[u,2]='German'\n",
    "#                 index+=1\n",
    "\n",
    "#             if(text_lang=='fr'):\n",
    "#                 if('product' in ureview_str or 'excellent' in ureview_str or 'mouse' in ureview_str):\n",
    "#                     df_aws_reviews.iloc[u,2]='English'\n",
    "#                 else:\n",
    "#                     df_aws_reviews.iloc[u,2]='French'\n",
    "#                 index+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # delete reviews that does not have language information\n",
    "#     # can also use the drop function from the pandas library\n",
    "#     df_aws_reviews=df_aws_reviews[df_aws_reviews['LANGUAGE'] != \"\"]\n",
    "#     df_aws_reviews.shape\n",
    "\n",
    "#     #functions to process the data:\n",
    "#     def count_words(sentence):\n",
    "#         words=word_tokenize(sentence)\n",
    "#         l=len(words)\n",
    "#         return l\n",
    "\n",
    "\n",
    "#     import langid\n",
    "#     def identify_lang(sentence):\n",
    "#         cl=langid.classify(sentence)\n",
    "#         lan=cl[0]\n",
    "#         return lan\n",
    "#     from nltk.stem import PorterStemmer\n",
    "#     from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#     ps = PorterStemmer()\n",
    "#     def stem_word(in_word):\n",
    "#         stemmed_word=ps.stem(in_word)\n",
    "#         return stemmed_word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #########################################################\n",
    "#     # division into the sentences\n",
    "#     df_num_of_rows=df_aws_reviews.shape[0]\n",
    "#     #mycolumns=['REVIEWSENTENCE_WID','REVIEW_WID','SENTENCE_ID','SKU','COUNTRY','SITE_URL','REVIEW_POSTED_DATE','WORD_COUNT',\n",
    "#     #                    'SENTIMENT','STAR_RATING','SENTENCE','PRODUCT_TYPE','PRODUCT_GROUP','PRODUCT_LINE_NAME']\n",
    "#     mycolumns_sentence=['textsentence_id','text_id','sentence_id','word_count',\n",
    "#                         'Aspects','sentence','language']\n",
    "#     df_aws_sentences=pd.DataFrame(columns=mycolumns_sentence)\n",
    "\n",
    "#     # adding sentences\n",
    "#     index_=0\n",
    "#     for i in range(0,df_num_of_rows):\n",
    "#         this_review=df_aws_reviews.iloc[i,1]\n",
    "#             # use try except because errpr was occuring in some cases\n",
    "#         try:\n",
    "#             sentences_this_review=sent_tokenize(this_review)\n",
    "#         except:\n",
    "#             print(\"Error in sentence tokenizing\")\n",
    "#             continue\n",
    "#         num_of_sents=len(sentences_this_review)\n",
    "#         current_review_id=str(df_aws_reviews.iloc[i,0])\n",
    "#         #print(current_review_id)\n",
    "#         if(num_of_sents!=0):\n",
    "#             sent_id=0\n",
    "#             for j in range(0,num_of_sents):\n",
    "#                 current_sentence=sentences_this_review[j]\n",
    "#                 if(current_sentence in [\"!\",\"?\",\".\"]):\n",
    "#                     continue\n",
    "#                 word_count=count_words(current_sentence)\n",
    "#                 reviewsentence_id=current_review_id+'_'+str(sent_id)#int(current_review_id+'_'+str(sent_id))\n",
    "#                 # Now calculate the polarity of sentece:\n",
    "#                 #sentiment_=get_sentiment(current_sentence)\n",
    "#                 if(df_aws_reviews.iloc[i,2]=='English'):\n",
    "#                     aspects_extracted=get_sentiment_en(current_sentence)\n",
    "#                 else:\n",
    "#                     print('Does not support other languages than English...')\n",
    "#                 ##### since some aspects may be repeated, we do the following:\n",
    "#                 ### we make the dictionary out of aspects and put that in the cell corresponding to the\n",
    "#                 ### or we just get the set values:\n",
    "#                 saspects_extracted=set(aspects_extracted) ## this will contain aspects only once.\n",
    "                \n",
    "#                 one_row=[reviewsentence_id,current_review_id,sent_id,word_count,saspects_extracted,current_sentence,df_aws_reviews.iloc[i,2]]\n",
    "#                 df_aws_sentences.loc[index_]=one_row\n",
    "#                 sent_id+=1\n",
    "#                 index_+=1\n",
    "\n",
    "                \n",
    " \n",
    "#     df_aws_sentences[['sentence_id','word_count']]=df_aws_sentences[['sentence_id','word_count']].astype(int)\n",
    "#     ########################################################\n",
    "#     ## now word frequency table\n",
    "\n",
    "#     cols_word_freq= ['reviewsentence_wid','review_wid','sentence_id','word','translated_word','freq']\n",
    "#     df_sents_num_of_rows=df_aws_sentences.shape[0]\n",
    "\n",
    "\n",
    "#     df_word_freq=pd.DataFrame(columns=cols_word_freq)\n",
    "#     # get rid of commas etc\n",
    "#     from nltk.tokenize import RegexpTokenizer\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#     import re\n",
    "#     def check_num(input_s): \n",
    "#         num_format = re.compile(\"^[\\-]?[1-9][0-9]*\\.?[0-9]+$\")\n",
    "#         isnumber = re.match(num_format,input_s)\n",
    "#         #isnumber=~\n",
    "#         if isnumber:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "#     def check_letter(input_s):\n",
    "#         #remove len 1 and 2s(come back for len 2 later)\n",
    "#         l=len(input_s)\n",
    "#         if(l==1 or l==2):\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "#     from nltk.corpus import stopwords\n",
    "#     #stopwords_ = set(stopwords.words('english'))\n",
    "#     stopwords_fr=set(stopwords.words('french'))\n",
    "#     stopwords_en=set(stopwords.words('english'))\n",
    "#     stopwords_ge=set(stopwords.words('german'))\n",
    "\n",
    "\n",
    "#     windex_=0\n",
    "\n",
    "#     for i in range(0,df_sents_num_of_rows):\n",
    "#         #print(i)\n",
    "#         sentence=df_aws_sentences.iloc[i,5]\n",
    "#         #words=word_tokenize(sentence)\n",
    "#         # maybe use try-except block as follows:\n",
    "#         #try:\n",
    "#         #words=tokenizer.tokenize(sentence)\n",
    "#         #except:\n",
    "#         #print(\"error in word tokenizing\")\n",
    "#         words=tokenizer.tokenize(sentence)\n",
    "#         tags_=nltk.pos_tag(words)\n",
    "#         num_words=len(words)\n",
    "#         for j in range(0,num_words):\n",
    "#             word=words[j]\n",
    "#             wordlow=word.lower()\n",
    "#             # check if it is noun here\n",
    "#             translated=wordlow  ## for another language we need translation\n",
    "#             freq=1\n",
    "#             w_isnum=check_num(wordlow)\n",
    "#             one_two_let=check_letter(wordlow)\n",
    "#             if(df_aws_sentences.iloc[i,6]=='English'):\n",
    "#                 stopwords_=stopwords_en\n",
    "#             if(df_aws_sentences.iloc[i,6]=='French'):\n",
    "#                 stopwords_=stopwords_fr\n",
    "#             if(df_aws_sentences.iloc[i,6]=='German'):\n",
    "#                 stopwords_=stopwords_ge\n",
    "#             if(wordlow in stopwords_ or w_isnum or one_two_let):\n",
    "#                 continue\n",
    "\n",
    "\n",
    "#             if(tags_[j][1]=='NN' or tags_[j][1]=='NNS' or tags_[j][1]=='NNP'):\n",
    "#                 # since we expect aspects are more likely to be among the nouns\n",
    "\n",
    "#                 one_row=[df_aws_sentences.iloc[i,0],df_aws_sentences.iloc[i,1],df_aws_sentences.iloc[i,2],wordlow,translated,freq]\n",
    "#                 df_word_freq.loc[windex_]=one_row\n",
    "#                 windex_+=1\n",
    "#             #print(windex_)\n",
    "\n",
    "#     df_word_freq[['sentence_id','freq']]=df_word_freq[['sentence_id','freq']].astype(int)\n",
    "#     df_word_freq.rename(columns={'reviewsentence_wid':'textsentence_id','review_wid':'text_id'},inplace=True)\n",
    "#     #######################################################\n",
    "#     # give indices a name\n",
    "#     #df.index.rename('Index')\n",
    "    \n",
    "#     ############################################################################\n",
    "#     ## output file generation and moving the Input file from Processed part\n",
    "#     ## this basically will be what the users need\n",
    "#     sentences_name_to_save=files_in_folder[ifiles][:-4]+'_ouput.csv'\n",
    "#     output_sentence_directory=output_directory+sentences_name_to_save\n",
    "#     df_aws_sentences.to_csv(output_sentence_directory,index=False)\n",
    "\n",
    "\n",
    "#     ###################################\n",
    "#     # merge 2 tables  -- in this part\n",
    "#     df_merged_output=df_aws_sentences.merge(df_word_freq,how='left',on=['textsentence_id','text_id','sentence_id'])\n",
    "#     merged_name_to_save=files_in_folder[ifiles][:-4]+'_output_words.csv'\n",
    "#     merged_output_directory=output_directory+merged_name_to_save\n",
    "#     df_merged_output.to_csv(merged_output_directory,index=False)\n",
    "\n",
    "#     # move processed file into folder named \n",
    "#     shutil.move(file_directory,processed_directory)\n",
    "\n",
    "\n",
    "#     ## 2 output files are generated -- \n",
    "#     # 1. sentences with sentiments\n",
    "#     # 2. merged table which contains word frequency-- this maybe used for visualisation in Tableau \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COPY of the aspect extraction engine code ::: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the code needed for running the sentiment analysis engine\n",
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "#from textblob import TextBlob,\n",
    "from textblob import Word, Blobber\n",
    "import textblob_fr\n",
    "import textblob_de\n",
    "import os\n",
    "import shutil # from removing files from one folder to another\n",
    "os.environ[\"NLS_LANG\"] = \".AL32UTF8\" \n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "############\n",
    "## the directories shown below should be changed when the new folder for the aspect extraction is given\n",
    "############\n",
    "\n",
    "import os\n",
    "input_directory='/home/ayaqubov/engines/Windows-share/Input/'\n",
    "files_in_folder=os.listdir(input_directory)\n",
    "output_directory='/home/ayaqubov/engines/Windows-share/Output/'\n",
    "processed_directory='/home/ayaqubov/engines/Windows-share/Processed/'\n",
    "\n",
    "\n",
    "# check while the input is empty do the \n",
    "\n",
    "#while(os.listdir('/home/ayaqubov/engines/Windows-share/Input/')!=\"\"):\n",
    "# get the data and process the data\n",
    "\n",
    "#functions to process the data:\n",
    "def count_words(sentence):\n",
    "    words=word_tokenize(sentence)\n",
    "    l=len(words)\n",
    "    return l\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ayaqubov/sentvec_new/italian/')\n",
    "from fasttext import FastVector\n",
    "en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "import numpy as np\n",
    "def find_min_distance(word,list_of_categories):\n",
    "    l=len(list_of_categories)\n",
    "    distances=[]\n",
    "    for i in range(0,l):\n",
    "        a=en_model[word]\n",
    "        b=en_model[list_of_categories[i]]\n",
    "        d=np.linalg.norm(a-b)\n",
    "        distances.append(d)\n",
    "    return distances.index(min(distances))\n",
    "\n",
    "def get_sentiment_en(sentence):\n",
    "    from textblob import TextBlob\n",
    "    # this function is the simplest function\n",
    "    blob=TextBlob(sentence)\n",
    "    sentiment_pol=blob.sentiment.polarity\n",
    "    #sentiment_sub=TextBlob(sentence).sentiment.subjectivity\n",
    "    #sentiment_=sentiment_pol\n",
    "    return sentiment_pol\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from textblob import Blobber\n",
    "def get_sentiment_fr(sentence):\n",
    "    mysentence=str(sentence)\n",
    "    tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "    sentim = tb(mysentence).sentiment\n",
    "    sentiment_pol=sentim[0]\n",
    "    sentiment_sub=sentim[1]\n",
    "    return sentiment_pol\n",
    "\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "from textblob_de import PatternTagger\n",
    "def get_sentiment_de(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    sentim=blob.sentiment\n",
    "    sentiment_pol=sentim[0]\n",
    "    sentiment_sub=sentim[1]\n",
    "    return sentiment_pol\n",
    "import langid\n",
    "def identify_lang(sentence):\n",
    "    cl=langid.classify(sentence)\n",
    "    lan=cl[0]\n",
    "    return lan\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "def stem_word(in_word):\n",
    "    stemmed_word=ps.stem(in_word)\n",
    "    return stemmed_word\n",
    "\n",
    "import langid\n",
    "def identify_lang(sentence):\n",
    "    cl=langid.classify(sentence)\n",
    "    lan=cl[0]\n",
    "    return lan\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "def stem_word(in_word):\n",
    "    stemmed_word=ps.stem(in_word)\n",
    "    return stemmed_word\n",
    "\n",
    "\n",
    "\n",
    "### aspect funcs.\n",
    "### all these functions below make up the aspect extraction from the sentence\n",
    "\n",
    "## This file contains functions needed to extract the aspects\n",
    "def word_tokens(mysentence):\n",
    "    # tokenization\n",
    "    import nltk\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #mysent='This camera is sleek, heavy and very affordable.' #I liked the book. Not to mention the price of the phone. '\n",
    "    #mysent='Love the sleekness of the player.'\n",
    "    #mysent='Very big to hold'\n",
    "    #mysent='Not to mention the price of the phone.'\n",
    "    #mysent='We ordered chicken casserole, but what we got were a few small pieces of chicken, all dark meat and on the bone.'\n",
    "    #mysentence='Logitech mouse was nice.'\n",
    "    mywords=tokenizer.tokenize(mysentence)\n",
    "    tags=nltk.pos_tag(mywords)\n",
    "    return tags\n",
    "\n",
    "def do_stemming(word):\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    st = LancasterStemmer()\n",
    "    stemmed=st.stem(word)\n",
    "    return stemmed\n",
    "\n",
    "def dependency_parse_sentence(mysentence):\n",
    "    from nltk.parse.stanford import StanfordDependencyParser\n",
    "    path_to_jar='/home/ayaqubov/stanford_nlp/stanford_parser/stanford-parser.jar'\n",
    "    path_to_models_jar='/home/ayaqubov/stanford_nlp/stanford_parser/stanford-parser-3.5.2-models.jar'\n",
    "    dependency_parser=StanfordDependencyParser(path_to_jar=path_to_jar,path_to_models_jar=path_to_models_jar)\n",
    "    result = dependency_parser.raw_parse(mysentence)\n",
    "    dep = result.next()\n",
    "    dependency_results=list(dep.triples())\n",
    "    #print \"Dependency results:::\"\n",
    "    #print dependency_results\n",
    "    return dependency_results\n",
    "\n",
    "\n",
    "def extract_explicit(mysentence):\n",
    "    ### finding explicit aspects based on the rules\n",
    "    dependency_results=dependency_parse_sentence(mysentence)\n",
    "    aspects_explicit=[]\n",
    "    ld=len(dependency_results)\n",
    "    for i in range(0,ld):\n",
    "        #rule 1\n",
    "        if(dependency_results[i][1] in ['amod','nmod','nummod','appos','det','nsubjpass']):# among nominal modifiers of the noun\n",
    "            if(dependency_results[i][2][1] in ['NN','NNP']):\n",
    "                asp=dependency_results[i][2][0]\n",
    "                aspects_explicit.append(asp)\n",
    "\n",
    "        # rule 1\n",
    "        if(dependency_results[i][1] in ['nsubj','dobj','iobj']): #adverbial or adjective modifier ---clausal argument relations\n",
    "            if(dependency_results[i][2][1] in ['NN','NNP'] and dependency_results[i][0][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "                asp=dependency_results[i][2][0]\n",
    "                aspects_explicit.append(asp)\n",
    "\n",
    "        # rule 2.\n",
    "        if(dependency_results[i][1] in ['nsubj','dobj','iobj']):\n",
    "            if(dependency_results[i][2][1] in ['NN','NNP'] and dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ']):\n",
    "                asp=dependency_results[i][2][0]\n",
    "                aspects_explicit.append(asp)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "    if(dependency_results[i][1]=='cop'):\n",
    "        if(dependency_results[i][0][1] in ['NNP','NNPS']):\n",
    "            aspects_explicit.append(dependency_results[i][0][1])\n",
    "    \n",
    "    \n",
    "        # rule 3\n",
    "\n",
    "    ##### sentences which do not have subject noun relation in their parse tree\n",
    "    #rule 3.3.4 .2\n",
    "    for ii in range(0,ld):\n",
    "        if(dependency_results[ii][1]=='case'):\n",
    "            mynoun=dependency_results[ii][0][0]\n",
    "            for kk in range(0,ld):\n",
    "                if(dependency_results[kk][2][0]==mynoun and dependency_results[kk][0][1] in ['NN','NNS','NNP','NNPS']):\n",
    "                    aspects_explicit.append(dependency_results[kk][0][0])\n",
    "                    if(dependency_results[ii][0][1] !='PRP'):\n",
    "                        aspects_explicit.append(mynoun)\n",
    "\n",
    "    ### among additional rules:\n",
    "    # rule 3.3.5. 2\n",
    "    for iii in range(0,ld):\n",
    "        if(dependency_results[iii][1]=='compound'):\n",
    "            if(dependency_results[iii][0][1] in ['NN','NNS','NNP','NNPS'] and dependency_results[iii][2][1] in ['NN','NNS','NNP','NNPS']):\n",
    "                new_aspect=dependency_results[iii][2][0]+' '+dependency_results[iii][0][0]\n",
    "                aspects_explicit.append(new_aspect)\n",
    "    unique_explicit_aspects=list(set(aspects_explicit))     \n",
    "    return unique_explicit_aspects\n",
    "\n",
    "def extract_implicit(mysentence):\n",
    "    dependency_results=dependency_parse_sentence(mysentence)\n",
    "    print dependency_results\n",
    "    ## getting the implicit aspects\n",
    "\n",
    "    implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "\n",
    "    implicit_aspects=[]\n",
    "    dl=len(dependency_results)\n",
    "    for i in range(0,dl):\n",
    "        if(dependency_results[i][1]=='cop'):## if the token is in copular relation with the copular verb and copular verb exists in implicit aspect lexicon\n",
    "            if(dependency_results[i][0][1] not in ['NNP','NNPS']):\n",
    "                implicit_indicator=dependency_results[i][0][0]\n",
    "                implicit_aspects.append(implicit_indicator)\n",
    "\n",
    "                for kk in range(0,len(dependency_results)):\n",
    "                    if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][1]=='conj'):\n",
    "                        #implicit_indicator=dependency_results[kk][2][0]\n",
    "                        implicit_aspects.append(dependency_results[kk][2][0])\n",
    "\n",
    "                    if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][2][1]=='VB'):\n",
    "                        implicit_aspects.append(dependency_results[kk][2][0])\n",
    "                        \n",
    "                        \n",
    "         ## check for copular verbs\n",
    "        copular_verbs=['is', 'am', 'are', 'was', 'were', 'appear', 'seem', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get']\n",
    "        ####Use copular verbs\n",
    "        #if the verb is modified by adjective and adverb:\n",
    "        if(dependency_results[i][1] in ['nsubj','dobj','iobj','ccomp','xcomp'] ):\n",
    "            if(dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ'] and dependency_results[i][2][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "                #if the noun is in subject relation verb\n",
    "                #take that verb:\n",
    "                myverb=dependency_results[i][0][0]\n",
    "                # search for the noun in that sentence\n",
    "                \n",
    "                stem_verb=do_stemming(myverb)\n",
    "                implicit_aspects.append(stem_verb)\n",
    "\n",
    "        if(dependency_results[i][1]=='amod'):\n",
    "            if(dependency_results[i][2][1]=='JJ' and dependency_results[i][0][1] in ['NN','NNS']):\n",
    "                implicit_aspects.append(dependency_results[i][2][0])\n",
    "                \n",
    "        if(dependency_results[i][1]=='nsubjpass'):\n",
    "            if(dependency_results[i][0][1] in ['JJ','JJR','JJS'] and dependency_results[i][2][1] in ['NN','NNS']):\n",
    "                implicit_aspects.append(dependency_results[i][0][0])\n",
    "\n",
    "    import gensim\n",
    "    from gensim.models import word2vec\n",
    "    \n",
    "    # get the pretrained word2vec model \n",
    "    #fname=\"/home/ayaqubov/AspectExtractor/myword2vec/word2vecmodel_electronics\"\n",
    "    #mymodel.save(fname)\n",
    "    # To get back the model\n",
    "    #mymodel = gensim.models.Word2Vec.load(fname)\n",
    "    \n",
    "    #en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "    #mymodel\n",
    "    #en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "    \n",
    "\n",
    "    # remove non-frequent implicit aspects from the sentences --->>> this is needed because we want to compare these to the categories\n",
    "    unique_implicit_aspects=list(set(implicit_aspects))\n",
    "    print 'Implicit aspects here:::'\n",
    "    print unique_implicit_aspects\n",
    "    \n",
    "    ### mapping of implicit aspect categories\n",
    "    implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "    list2=[]\n",
    "    \n",
    "    \n",
    "    output_categories=[]\n",
    "    for k in range(0,len(unique_implicit_aspects)):\n",
    "        asp_=unique_implicit_aspects[k]\n",
    "        \n",
    "        try:\n",
    "            a=find_min_distance(asp_,implicit_aspect_categories)\n",
    "            output_categories.append(implicit_aspect_categories[a])\n",
    "        except:\n",
    "            print \"Implicit aspect directly goes to output ... Model does not contain the word.\"\n",
    "            output_categories.append(asp_)\n",
    "            \n",
    "        \n",
    "    \n",
    "    return output_categories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def importance_of_aspects(aspects_explicit):\n",
    "    #printing the importance of the explicit aspects\n",
    "    from collections import Counter\n",
    "    counter_aspects=Counter(aspects_explicit)\n",
    "    print(counter_aspects)\n",
    "\n",
    "def find_max_index(mylist):\n",
    "        import numpy as np\n",
    "        ind = np.argmax(mylist)\n",
    "        return ind\n",
    "\n",
    "## this is with different kind of model\n",
    "# def extract_implicit(mysentence):\n",
    "#     dependency_results=dependency_parse_sentence(mysentence)\n",
    "#     print dependency_results\n",
    "#     ## getting the implicit aspects\n",
    "\n",
    "#     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "\n",
    "#     implicit_aspects=[]\n",
    "#     dl=len(dependency_results)\n",
    "#     for i in range(0,dl):\n",
    "#         if(dependency_results[i][1]=='cop'):## if the token is in copular relation with the copular verb and copular verb exists in implicit aspect lexicon\n",
    "#             if(dependency_results[i][0][1] not in ['NNP','NNPS']):\n",
    "#                 implicit_indicator=dependency_results[i][0][0]\n",
    "#                 implicit_aspects.append(implicit_indicator)\n",
    "\n",
    "#                 for kk in range(0,len(dependency_results)):\n",
    "#                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][1]=='conj'):\n",
    "#                         #implicit_indicator=dependency_results[kk][2][0]\n",
    "#                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "\n",
    "#                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][2][1]=='VB'):\n",
    "#                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "                        \n",
    "                        \n",
    "#          ## check for copular verbs\n",
    "#         copular_verbs=['is', 'am', 'are', 'was', 'were', 'appear', 'seem', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get']\n",
    "#         ####Use copular verbs\n",
    "#         #if the verb is modified by adjective and adverb:\n",
    "#         if(dependency_results[i][1] in ['nsubj','dobj','iobj','ccomp','xcomp'] ):\n",
    "#             if(dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ'] and dependency_results[i][2][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "#                 #if the noun is in subject relation verb\n",
    "#                 #take that verb:\n",
    "#                 myverb=dependency_results[i][0][0]\n",
    "#                 # search for the noun in that sentence\n",
    "                \n",
    "#                 stem_verb=do_stemming(myverb)\n",
    "#                 implicit_aspects.append(stem_verb)\n",
    "\n",
    "#         if(dependency_results[i][1]=='amod'):\n",
    "#             if(dependency_results[i][2][1]=='JJ' and dependency_results[i][0][1] in ['NN','NNS']):\n",
    "#                 implicit_aspects.append(dependency_results[i][2][0])\n",
    "                \n",
    "#         if(dependency_results[i][1]=='nsubjpass'):\n",
    "#             if(dependency_results[i][0][1] in ['JJ','JJR','JJS'] and dependency_results[i][2][1] in ['NN','NNS']):\n",
    "#                 implicit_aspects.append(dependency_results[i][0][0])\n",
    "\n",
    "#     import gensim\n",
    "#     from gensim.models import word2vec\n",
    "\n",
    "#     # get the pretrained word2vec model \n",
    "#     fname=\"/home/ayaqubov/AspectExtractor/myword2vec/word2vecmodel_electronics\"\n",
    "#     #mymodel.save(fname)\n",
    "#     # To get back the model\n",
    "#     mymodel = gensim.models.Word2Vec.load(fname)\n",
    "    \n",
    "#     # remove non-frequent implicit aspects from the sentences --->>> this is needed because we want to compare these to the categories\n",
    "#     unique_implicit_aspects=list(set(implicit_aspects))\n",
    "#     print 'Implicit aspects here:::'\n",
    "#     print unique_implicit_aspects\n",
    "    \n",
    "#     ### mapping of implicit aspect categories\n",
    "#     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "#     list2=[]\n",
    "\n",
    "#     for i in range(0,len(unique_implicit_aspects)):\n",
    "#         list2.append(unique_implicit_aspects[i])\n",
    "\n",
    "#     similarities=[]\n",
    "#     for word2 in list2:\n",
    "#         s=[]\n",
    "#         for asp_cat in implicit_aspect_categories:\n",
    "#             try:\n",
    "#                 s.append(mymodel.similarity(word2,asp_cat))\n",
    "#             except:\n",
    "#                 print(\"This word was not found -- improvement with fasttext\")\n",
    "#                 s.append(word2)\n",
    "#         similarities.append(s)\n",
    "\n",
    "#     max_indices=[]\n",
    "#     for i in range(0,len(similarities)):\n",
    "#         m=find_max_index(similarities[i])\n",
    "#         max_indices.append(m)\n",
    "\n",
    "#     # this part does aspect category paring using the results from the WORD2VEC\n",
    "#     aspect_category_pair = dict()\n",
    "#     for i in range(0,len(unique_implicit_aspects)):\n",
    "#         max_l=max_indices[i]\n",
    "#         aspect_category_pair[unique_implicit_aspects[i]]=implicit_aspect_categories[max_l]\n",
    "\n",
    "#     #combining implicit and explicit aspects\n",
    "#     #total_aspects=aspects_explicit\n",
    "#     implicit_categories=aspect_category_pair.values()\n",
    "#     return implicit_categories\n",
    "\n",
    "\n",
    "def combine_aspects(aspects_explicit,implicit_categories):\n",
    "    total_aspects=aspects_explicit\n",
    "    \n",
    "    for ij in range(0,len(implicit_categories)):\n",
    "        total_aspects.append(implicit_categories[ij])\n",
    "    #here are the total aspects\n",
    "    # these aspects represnet the sentence\n",
    "    #print(total_aspects)\n",
    "    #now issue is to find the overall \n",
    "    return total_aspects\n",
    "    total_aspects=aspects_explicit\n",
    "    \n",
    "    return total_aspects\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#2 ways to run : either run the main function or run the get_aspects function\n",
    "\n",
    "########################\n",
    "\n",
    "def get_aspects(mysentence):\n",
    "    import os\n",
    "    from nltk.parse import stanford\n",
    "    import os\n",
    "    java_path = \"/usr/lib/jvm/jre-1.8.0\"\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "    myimplicit=extract_implicit(mysentence)\n",
    "    myexplicit=extract_explicit(mysentence)\n",
    "    total_aspects=combine_aspects(myexplicit,myimplicit)\n",
    "    if(total_aspects==[]): # when we dont have specific aspect in the sentence, we add 'GENERAL'\n",
    "        total_aspects.append('GENERAL')\n",
    "    return total_aspects\n",
    "\n",
    "\n",
    "\n",
    "for ifiles in range(0,len(files_in_folder)):\n",
    "    file_directory=input_directory + files_in_folder[ifiles]\n",
    "    print(file_directory)\n",
    "    #file_directory\n",
    "    mycolumnsdf=['REVIEW_WID','REVIEW_FULL_TEXT']\n",
    "    df_aws_reviews=pd.DataFrame(columns=mycolumnsdf)\n",
    "    df_aws_reviews=pd.read_csv(file_directory)\n",
    "    df_aws_reviews['LANGUAGE']=\"\"\n",
    "    num_unique_reviews=df_aws_reviews.shape[0]\n",
    "\n",
    "    ###########################################################\n",
    "    # here comes the check the language part and write correspondingly to the df_aws_reviews dataframe\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    ## data processing\n",
    "    print('Processing...')\n",
    "    index=0\n",
    "    \n",
    "    for u in range(0,num_unique_reviews):\n",
    "        review_id=df_aws_reviews.iloc[u,0]\n",
    "        print(review_id)\n",
    "        review_text=df_aws_reviews.iloc[u,1]\n",
    "        review_str=str(review_text)\n",
    "        #review_strw=review_str.encode('cp1252')## windows encoded\n",
    "        #ureview_str=review_strw.decode('utf-8')\n",
    "        #ureview_str=unicode(review_str,\"utf-8\")\n",
    "        ureview_str=review_str.decode('cp1252').encode('utf-8',errors='ignore')\n",
    "        #print(review_str)\n",
    "        #print(len(review_str))\n",
    "        contain_l=re.search('[a-zA-Z]', review_str)\n",
    "        if(contain_l!='None'):\n",
    "            # handle japanese,arabic,chinese cases because they appear as the ?? marks in the results\n",
    "            try:\n",
    "                text_lang=detect(ureview_str)\n",
    "                #text_lang2=identify_lang(review_str)\n",
    "            except:\n",
    "                print 'Error in reading, keep reading'\n",
    "                #i_debug+=1\n",
    "                continue\n",
    "            #check_words=word_tokenize(review_str)\n",
    "            #for iiii in range(0,len(check_the_words)):\n",
    "            #    if (check_the_words[iiii]=='product' or check_the_words[iiii]=='excellent'  or check_the_words[iiii]=='mouse'):\n",
    "            #        break\n",
    "            #            #continue\n",
    "\n",
    "            if(text_lang=='en'):\n",
    "                df_aws_reviews.iloc[u,2]='English'\n",
    "                index+=1\n",
    "\n",
    "            if(text_lang=='de'):\n",
    "                df_aws_reviews.iloc[u,2]='German'\n",
    "                index+=1\n",
    "\n",
    "            if(text_lang=='fr'):\n",
    "                if('product' in ureview_str or 'excellent' in ureview_str or 'mouse' in ureview_str):\n",
    "                    df_aws_reviews.iloc[u,2]='English'\n",
    "                else:\n",
    "                    df_aws_reviews.iloc[u,2]='French'\n",
    "                index+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # delete reviews that does not have language information\n",
    "    # can also use the drop function from the pandas library\n",
    "    df_aws_reviews=df_aws_reviews[df_aws_reviews['LANGUAGE'] != \"\"]\n",
    "    df_aws_reviews.shape\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # division into the sentences\n",
    "    df_num_of_rows=df_aws_reviews.shape[0]\n",
    "    #mycolumns=['REVIEWSENTENCE_WID','REVIEW_WID','SENTENCE_ID','SKU','COUNTRY','SITE_URL','REVIEW_POSTED_DATE','WORD_COUNT',\n",
    "    #                    'SENTIMENT','STAR_RATING','SENTENCE','PRODUCT_TYPE','PRODUCT_GROUP','PRODUCT_LINE_NAME']\n",
    "    mycolumns_sentence=['textsentence_id','text_id','sentence_id','word_count',\n",
    "                        'Aspects','sentence','language']\n",
    "    df_aws_sentences=pd.DataFrame(columns=mycolumns_sentence)\n",
    "\n",
    "    # adding sentences\n",
    "    index_=0\n",
    "    for i in range(0,df_num_of_rows):\n",
    "        print(\"i is \",i)\n",
    "        this_review=df_aws_reviews.iloc[i,1]\n",
    "            # use try except because errpr was occuring in some cases\n",
    "        try:\n",
    "            sentences_this_review=sent_tokenize(this_review)\n",
    "        except:\n",
    "            print(\"Error in sentence tokenizing\")\n",
    "            continue\n",
    "        num_of_sents=len(sentences_this_review)\n",
    "        current_review_id=str(df_aws_reviews.iloc[i,0])\n",
    "        #print(current_review_id)\n",
    "        if(num_of_sents!=0):\n",
    "            sent_id=0\n",
    "            for j in range(0,num_of_sents):\n",
    "                current_sentence=sentences_this_review[j]\n",
    "                if(current_sentence in [\"!\",\"?\",\".\"]):\n",
    "                    continue\n",
    "                word_count=count_words(current_sentence)\n",
    "                reviewsentence_id=current_review_id+'_'+str(sent_id)#int(current_review_id+'_'+str(sent_id))\n",
    "                # Now calculate the polarity of sentece:\n",
    "                #sentiment_=get_sentiment(current_sentence)\n",
    "                if(df_aws_reviews.iloc[i,2]=='English'):\n",
    "                    aspects_extracted=get_aspects(current_sentence)\n",
    "                else:\n",
    "                    print('Does not support other languages than English...')\n",
    "                ##### since some aspects may be repeated, we do the following:\n",
    "                ### we make the dictionary out of aspects and put that in the cell corresponding to the\n",
    "                ### or we just get the set values:\n",
    "                print(aspects_extracted)\n",
    "                saspects_extracted=list(set(aspects_extracted)) ## this will contain aspects only once.\n",
    "                \n",
    "                one_row=[reviewsentence_id,current_review_id,sent_id,word_count,saspects_extracted,current_sentence,df_aws_reviews.iloc[i,2]]\n",
    "                df_aws_sentences.loc[index_]=one_row\n",
    "                sent_id+=1\n",
    "                index_+=1\n",
    "\n",
    "                \n",
    " \n",
    "    df_aws_sentences[['sentence_id','word_count']]=df_aws_sentences[['sentence_id','word_count']].astype(int)\n",
    "    ########################################################\n",
    "    ## now word frequency table\n",
    "\n",
    "    cols_word_freq= ['reviewsentence_wid','review_wid','sentence_id','word','translated_word','freq']\n",
    "    df_sents_num_of_rows=df_aws_sentences.shape[0]\n",
    "\n",
    "\n",
    "    df_word_freq=pd.DataFrame(columns=cols_word_freq)\n",
    "    # get rid of commas etc\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    import re\n",
    "    def check_num(input_s): \n",
    "        num_format = re.compile(\"^[\\-]?[1-9][0-9]*\\.?[0-9]+$\")\n",
    "        isnumber = re.match(num_format,input_s)\n",
    "        #isnumber=~\n",
    "        if isnumber:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def check_letter(input_s):\n",
    "        #remove len 1 and 2s(come back for len 2 later)\n",
    "        l=len(input_s)\n",
    "        if(l==1 or l==2):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    #stopwords_ = set(stopwords.words('english'))\n",
    "    stopwords_fr=set(stopwords.words('french'))\n",
    "    stopwords_en=set(stopwords.words('english'))\n",
    "    stopwords_ge=set(stopwords.words('german'))\n",
    "\n",
    "\n",
    "    windex_=0\n",
    "\n",
    "    for i in range(0,df_sents_num_of_rows):\n",
    "        #print(i)\n",
    "        sentence=df_aws_sentences.iloc[i,5]\n",
    "        #words=word_tokenize(sentence)\n",
    "        # maybe use try-except block as follows:\n",
    "        #try:\n",
    "        #words=tokenizer.tokenize(sentence)\n",
    "        #except:\n",
    "        #print(\"error in word tokenizing\")\n",
    "        words=tokenizer.tokenize(sentence)\n",
    "        tags_=nltk.pos_tag(words)\n",
    "        num_words=len(words)\n",
    "        for j in range(0,num_words):\n",
    "            word=words[j]\n",
    "            wordlow=word.lower()\n",
    "            # check if it is noun here\n",
    "            translated=wordlow  ## for another language we need translation\n",
    "            freq=1\n",
    "            w_isnum=check_num(wordlow)\n",
    "            one_two_let=check_letter(wordlow)\n",
    "            if(df_aws_sentences.iloc[i,6]=='English'):\n",
    "                stopwords_=stopwords_en\n",
    "            if(df_aws_sentences.iloc[i,6]=='French'):\n",
    "                stopwords_=stopwords_fr\n",
    "            if(df_aws_sentences.iloc[i,6]=='German'):\n",
    "                stopwords_=stopwords_ge\n",
    "            if(wordlow in stopwords_ or w_isnum or one_two_let):\n",
    "                continue\n",
    "\n",
    "\n",
    "            if(tags_[j][1]=='NN' or tags_[j][1]=='NNS' or tags_[j][1]=='NNP'):\n",
    "                # since we expect aspects are more likely to be among the nouns\n",
    "\n",
    "                one_row=[df_aws_sentences.iloc[i,0],df_aws_sentences.iloc[i,1],df_aws_sentences.iloc[i,2],wordlow,translated,freq]\n",
    "                df_word_freq.loc[windex_]=one_row\n",
    "                windex_+=1\n",
    "            #print(windex_)\n",
    "\n",
    "    df_word_freq[['sentence_id','freq']]=df_word_freq[['sentence_id','freq']].astype(int)\n",
    "    df_word_freq.rename(columns={'reviewsentence_wid':'textsentence_id','review_wid':'text_id'},inplace=True)\n",
    "    #######################################################\n",
    "    # give indices a name\n",
    "    #df.index.rename('Index')\n",
    "    \n",
    "    ############################################################################\n",
    "    ## output file generation and moving the Input file from Processed part\n",
    "    ## this basically will be what the users need\n",
    "    sentences_name_to_save=files_in_folder[ifiles][:-4]+'_ouput.csv'\n",
    "    output_sentence_directory=output_directory+sentences_name_to_save\n",
    "    df_aws_sentences.to_csv(output_sentence_directory,index=False)\n",
    "\n",
    "\n",
    "    ###################################\n",
    "    # merge 2 tables  -- in this part\n",
    "    df_merged_output=df_aws_sentences.merge(df_word_freq,how='left',on=['textsentence_id','text_id','sentence_id'])\n",
    "    merged_name_to_save=files_in_folder[ifiles][:-4]+'_output_aspect.csv'\n",
    "    merged_output_directory=output_directory+merged_name_to_save\n",
    "    df_merged_output.to_csv(merged_output_directory,index=False)\n",
    "\n",
    "    # move processed file into folder named \n",
    "    shutil.move(file_directory,processed_directory)\n",
    "\n",
    "\n",
    "    ## 2 output files are generated -- \n",
    "    # 1. sentences with sentiments\n",
    "    # 2. merged table which contains word frequency-- this maybe used for visualisation in Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'i' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6a9ec51334c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0;31m#sentiment_=get_sentiment(current_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_aws_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'English'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                     \u001b[0maspects_extracted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_aspects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Does not support other languages than English...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6a9ec51334c2>\u001b[0m in \u001b[0;36mget_aspects\u001b[0;34m(mysentence)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JAVAHOME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjava_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mmyimplicit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_implicit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmysentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mmyexplicit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_explicit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmysentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m     \u001b[0mtotal_aspects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombine_aspects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyexplicit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyimplicit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_aspects\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# when we dont have specific aspect in the sentence, we add 'GENERAL'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6a9ec51334c2>\u001b[0m in \u001b[0;36mextract_explicit\u001b[0;34m(mysentence)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'cop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'NNP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NNPS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mis_from_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'i' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# this is the code needed for running the sentiment analysis engine\n",
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "#from textblob import TextBlob,\n",
    "from textblob import Word, Blobber\n",
    "import textblob_fr\n",
    "import textblob_de\n",
    "import os\n",
    "import shutil # from removing files from one folder to another\n",
    "os.environ[\"NLS_LANG\"] = \".AL32UTF8\" \n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "############\n",
    "## the directories shown below should be changed when the new folder for the aspect extraction is given\n",
    "############\n",
    "\n",
    "import os\n",
    "input_directory='/home/ayaqubov/engines/Windows-share/Input/'\n",
    "files_in_folder=os.listdir(input_directory)\n",
    "output_directory='/home/ayaqubov/engines/Windows-share/Output/'\n",
    "processed_directory='/home/ayaqubov/engines/Windows-share/Processed/'\n",
    "\n",
    "\n",
    "# check while the input is empty do the \n",
    "\n",
    "#while(os.listdir('/home/ayaqubov/engines/Windows-share/Input/')!=\"\"):\n",
    "# get the data and process the data\n",
    "\n",
    "#functions to process the data:\n",
    "def count_words(sentence):\n",
    "    words=word_tokenize(sentence)\n",
    "    l=len(words)\n",
    "    return l\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ayaqubov/sentvec_new/italian/')\n",
    "from fasttext import FastVector\n",
    "en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "import numpy as np\n",
    "def find_min_distance(word,list_of_categories):\n",
    "    l=len(list_of_categories)\n",
    "    distances=[]\n",
    "    for i in range(0,l):\n",
    "        a=en_model[word]\n",
    "        b=en_model[list_of_categories[i]]\n",
    "        d=np.linalg.norm(a-b)\n",
    "        distances.append(d)\n",
    "    return distances.index(min(distances))\n",
    "\n",
    "def get_sentiment_en(sentence):\n",
    "    from textblob import TextBlob\n",
    "    # this function is the simplest function\n",
    "    blob=TextBlob(sentence)\n",
    "    sentiment_pol=blob.sentiment.polarity\n",
    "    #sentiment_sub=TextBlob(sentence).sentiment.subjectivity\n",
    "    #sentiment_=sentiment_pol\n",
    "    return sentiment_pol\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from textblob import Blobber\n",
    "def get_sentiment_fr(sentence):\n",
    "    mysentence=str(sentence)\n",
    "    tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "    sentim = tb(mysentence).sentiment\n",
    "    sentiment_pol=sentim[0]\n",
    "    sentiment_sub=sentim[1]\n",
    "    return sentiment_pol\n",
    "\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "from textblob_de import PatternTagger\n",
    "def get_sentiment_de(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    sentim=blob.sentiment\n",
    "    sentiment_pol=sentim[0]\n",
    "    sentiment_sub=sentim[1]\n",
    "    return sentiment_pol\n",
    "import langid\n",
    "def identify_lang(sentence):\n",
    "    cl=langid.classify(sentence)\n",
    "    lan=cl[0]\n",
    "    return lan\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "def stem_word(in_word):\n",
    "    stemmed_word=ps.stem(in_word)\n",
    "    return stemmed_word\n",
    "\n",
    "import langid\n",
    "def identify_lang(sentence):\n",
    "    cl=langid.classify(sentence)\n",
    "    lan=cl[0]\n",
    "    return lan\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "def stem_word(in_word):\n",
    "    stemmed_word=ps.stem(in_word)\n",
    "    return stemmed_word\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords_ = set(stopwords.words('english'))\n",
    "stopwords_fr=set(stopwords.words('french'))\n",
    "stopwords_en=set(stopwords.words('english'))\n",
    "stopwords_ge=set(stopwords.words('german'))\n",
    "\n",
    "\n",
    "def is_from_stopwords(asp):\n",
    "    lasp=asp.lower()\n",
    "    if(lasp in stopwords_en or lasp in stopwords_fr or lasp in stopwords_ge):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "### aspect funcs.\n",
    "### all these functions below make up the aspect extraction from the sentence\n",
    "\n",
    "## This file contains functions needed to extract the aspects\n",
    "def word_tokens(mysentence):\n",
    "    # tokenization\n",
    "    import nltk\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #mysent='This camera is sleek, heavy and very affordable.' #I liked the book. Not to mention the price of the phone. '\n",
    "    #mysent='Love the sleekness of the player.'\n",
    "    #mysent='Very big to hold'\n",
    "    #mysent='Not to mention the price of the phone.'\n",
    "    #mysent='We ordered chicken casserole, but what we got were a few small pieces of chicken, all dark meat and on the bone.'\n",
    "    #mysentence='Logitech mouse was nice.'\n",
    "    mywords=tokenizer.tokenize(mysentence)\n",
    "    tags=nltk.pos_tag(mywords)\n",
    "    return tags\n",
    "\n",
    "def do_stemming(word):\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    st = LancasterStemmer()\n",
    "    stemmed=st.stem(word)\n",
    "    return stemmed\n",
    "\n",
    "def dependency_parse_sentence(mysentence):\n",
    "    from nltk.parse.stanford import StanfordDependencyParser\n",
    "    path_to_jar='/home/ayaqubov/stanford_nlp/stanford_parser/stanford-parser.jar'\n",
    "    path_to_models_jar='/home/ayaqubov/stanford_nlp/stanford_parser/stanford-parser-3.5.2-models.jar'\n",
    "    dependency_parser=StanfordDependencyParser(path_to_jar=path_to_jar,path_to_models_jar=path_to_models_jar)\n",
    "    result = dependency_parser.raw_parse(mysentence)\n",
    "    dep = result.next()\n",
    "    dependency_results=list(dep.triples())\n",
    "    #print \"Dependency results:::\"\n",
    "    #print dependency_results\n",
    "    return dependency_results\n",
    "\n",
    "\n",
    "def extract_explicit(mysentence):\n",
    "    ### finding explicit aspects based on the rules\n",
    "    dependency_results=dependency_parse_sentence(mysentence)\n",
    "    aspects_explicit=[]\n",
    "    ld=len(dependency_results)\n",
    "    for i in range(0,ld):\n",
    "        #rule 1\n",
    "        if(dependency_results[i][1] in ['amod','nmod','nummod','appos','det','nsubjpass']):# among nominal modifiers of the noun\n",
    "            if(dependency_results[i][2][1] in ['NN','NNP']):\n",
    "                asp=dependency_results[i][2][0]\n",
    "                if(not is_from_stopwords(asp)):\n",
    "                    aspects_explicit.append(asp)\n",
    "\n",
    "        # rule 1\n",
    "        if(dependency_results[i][1] in ['nsubj','dobj','iobj']): #adverbial or adjective modifier ---clausal argument relations\n",
    "            if(dependency_results[i][2][1] in ['NN','NNP'] and dependency_results[i][0][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "                asp=dependency_results[i][2][0]\n",
    "                if(not is_from_stopwords(asp)):\n",
    "                    aspects_explicit.append(asp)\n",
    "\n",
    "        # rule 2.\n",
    "        if(dependency_results[i][1] in ['nsubj','dobj','iobj']):\n",
    "            if(dependency_results[i][2][1] in ['NN','NNP'] and dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ']):\n",
    "                asp=dependency_results[i][2][0]\n",
    "                if(not is_from_stopwords(asp)):\n",
    "                    aspects_explicit.append(asp)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "    if(dependency_results[i][1]=='cop'):\n",
    "        if(dependency_results[i][0][1] in ['NNP','NNPS']):\n",
    "            if(not is_from_stopwords(dependency_results[i][0][1])):\n",
    "                if(not is_from_stopwords(dependency_results[i][0][1])):\n",
    "                    aspects_explicit.append(dependency_results[i][0][1])\n",
    "    \n",
    "    \n",
    "        # rule 3\n",
    "\n",
    "    ##### sentences which do not have subject noun relation in their parse tree\n",
    "    #rule 3.3.4 .2\n",
    "    for ii in range(0,ld):\n",
    "        if(dependency_results[ii][1]=='case'):\n",
    "            mynoun=dependency_results[ii][0][0]\n",
    "            for kk in range(0,ld):\n",
    "                if(dependency_results[kk][2][0]==mynoun and dependency_results[kk][0][1] in ['NN','NNS','NNP','NNPS']):\n",
    "                    aspects_explicit.append(dependency_results[kk][0][0])\n",
    "                    if(dependency_results[ii][0][1] !='PRP'):\n",
    "                        if(not is_from_stopwords(mynoun)):\n",
    "                            aspects_explicit.append(mynoun)\n",
    "\n",
    "    ### among additional rules:\n",
    "    # rule 3.3.5. 2\n",
    "    for iii in range(0,ld):\n",
    "        if(dependency_results[iii][1]=='compound'):\n",
    "            if(dependency_results[iii][0][1] in ['NN','NNS','NNP','NNPS'] and dependency_results[iii][2][1] in ['NN','NNS','NNP','NNPS']):\n",
    "                new_aspect=dependency_results[iii][2][0]+' '+dependency_results[iii][0][0]\n",
    "                if(not is_from_stopwords(new_aspect)):\n",
    "                    aspects_explicit.append(new_aspect)\n",
    "    unique_explicit_aspects=list(set(aspects_explicit))     \n",
    "    return unique_explicit_aspects\n",
    "\n",
    "def extract_implicit(mysentence):\n",
    "    dependency_results=dependency_parse_sentence(mysentence)\n",
    "    print dependency_results\n",
    "    ## getting the implicit aspects\n",
    "\n",
    "    implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "\n",
    "    implicit_aspects=[]\n",
    "    dl=len(dependency_results)\n",
    "    for i in range(0,dl):\n",
    "        if(dependency_results[i][1]=='cop'):## if the token is in copular relation with the copular verb and copular verb exists in implicit aspect lexicon\n",
    "            if(dependency_results[i][0][1] not in ['NNP','NNPS']):\n",
    "                implicit_indicator=dependency_results[i][0][0]\n",
    "                if(not is_from_stopwords(implicit_indicator)):\n",
    "                    implicit_aspects.append(implicit_indicator)\n",
    "\n",
    "                for kk in range(0,len(dependency_results)):\n",
    "                    if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][1]=='conj'):\n",
    "                        #implicit_indicator=dependency_results[kk][2][0]\n",
    "                        if(not is_from_stopwords(dependency_results[kk][2][0])):\n",
    "                            implicit_aspects.append(dependency_results[kk][2][0])\n",
    "\n",
    "                    if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][2][1]=='VB'):\n",
    "                        if(not is_from_stopwords(dependency_results[kk][2][0])):\n",
    "                            implicit_aspects.append(dependency_results[kk][2][0])\n",
    "                        \n",
    "                        \n",
    "         ## check for copular verbs\n",
    "        copular_verbs=['is', 'am', 'are', 'was', 'were', 'appear', 'seem', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get']\n",
    "        ####Use copular verbs\n",
    "        #if the verb is modified by adjective and adverb:\n",
    "        if(dependency_results[i][1] in ['nsubj','dobj','iobj','ccomp','xcomp'] ):\n",
    "            if(dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ'] and dependency_results[i][2][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "                #if the noun is in subject relation verb\n",
    "                #take that verb:\n",
    "                myverb=dependency_results[i][0][0]\n",
    "                # search for the noun in that sentence\n",
    "                \n",
    "                stem_verb=do_stemming(myverb)\n",
    "                if(not is_from_stopwords(stem_verb)):\n",
    "                    implicit_aspects.append(stem_verb)\n",
    "\n",
    "        if(dependency_results[i][1]=='amod'):\n",
    "            if(dependency_results[i][2][1]=='JJ' and dependency_results[i][0][1] in ['NN','NNS']):\n",
    "                if(not is_from_stopwords(dependency_results[i][2][0])):\n",
    "                    implicit_aspects.append(dependency_results[i][2][0])\n",
    "                \n",
    "        if(dependency_results[i][1]=='nsubjpass'):\n",
    "            if(dependency_results[i][0][1] in ['JJ','JJR','JJS'] and dependency_results[i][2][1] in ['NN','NNS']):\n",
    "                if(not is_from_stopwords(dependency_results[i][0][0])):\n",
    "                    implicit_aspects.append(dependency_results[i][0][0])\n",
    "\n",
    "    import gensim\n",
    "    from gensim.models import word2vec\n",
    "    \n",
    "    # get the pretrained word2vec model \n",
    "    #fname=\"/home/ayaqubov/AspectExtractor/myword2vec/word2vecmodel_electronics\"\n",
    "    #mymodel.save(fname)\n",
    "    # To get back the model\n",
    "    #mymodel = gensim.models.Word2Vec.load(fname)\n",
    "    \n",
    "    #en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "    #mymodel\n",
    "    #en_model=FastVector(vector_file='/home/ayaqubov/sentvec_new/fasttext_embeddings/eng/wiki.en.vec')\n",
    "    \n",
    "\n",
    "    # remove non-frequent implicit aspects from the sentences --->>> this is needed because we want to compare these to the categories\n",
    "    unique_implicit_aspects=list(set(implicit_aspects))\n",
    "    print 'Implicit aspects here:::'\n",
    "    print unique_implicit_aspects\n",
    "    \n",
    "    ### mapping of implicit aspect categories\n",
    "    implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "    list2=[]\n",
    "    \n",
    "    \n",
    "    output_categories=[]\n",
    "    for k in range(0,len(unique_implicit_aspects)):\n",
    "        asp_=unique_implicit_aspects[k]\n",
    "        \n",
    "        try:\n",
    "            a=find_min_distance(asp_,implicit_aspect_categories)\n",
    "            output_categories.append(implicit_aspect_categories[a])\n",
    "        except:\n",
    "            print \"Implicit aspect directly goes to output ... Model does not contain the word.\"\n",
    "            output_categories.append(asp_)\n",
    "            \n",
    "        \n",
    "    \n",
    "    return output_categories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def importance_of_aspects(aspects_explicit):\n",
    "    #printing the importance of the explicit aspects\n",
    "    from collections import Counter\n",
    "    counter_aspects=Counter(aspects_explicit)\n",
    "    print(counter_aspects)\n",
    "\n",
    "def find_max_index(mylist):\n",
    "        import numpy as np\n",
    "        ind = np.argmax(mylist)\n",
    "        return ind\n",
    "\n",
    "## this is with different kind of model\n",
    "# def extract_implicit(mysentence):\n",
    "#     dependency_results=dependency_parse_sentence(mysentence)\n",
    "#     print dependency_results\n",
    "#     ## getting the implicit aspects\n",
    "\n",
    "#     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "\n",
    "#     implicit_aspects=[]\n",
    "#     dl=len(dependency_results)\n",
    "#     for i in range(0,dl):\n",
    "#         if(dependency_results[i][1]=='cop'):## if the token is in copular relation with the copular verb and copular verb exists in implicit aspect lexicon\n",
    "#             if(dependency_results[i][0][1] not in ['NNP','NNPS']):\n",
    "#                 implicit_indicator=dependency_results[i][0][0]\n",
    "#                 implicit_aspects.append(implicit_indicator)\n",
    "\n",
    "#                 for kk in range(0,len(dependency_results)):\n",
    "#                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][1]=='conj'):\n",
    "#                         #implicit_indicator=dependency_results[kk][2][0]\n",
    "#                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "\n",
    "#                     if(dependency_results[kk][0][0]==implicit_indicator and dependency_results[kk][2][1]=='VB'):\n",
    "#                         implicit_aspects.append(dependency_results[kk][2][0])\n",
    "                        \n",
    "                        \n",
    "#          ## check for copular verbs\n",
    "#         copular_verbs=['is', 'am', 'are', 'was', 'were', 'appear', 'seem', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get']\n",
    "#         ####Use copular verbs\n",
    "#         #if the verb is modified by adjective and adverb:\n",
    "#         if(dependency_results[i][1] in ['nsubj','dobj','iobj','ccomp','xcomp'] ):\n",
    "#             if(dependency_results[i][0][1] in ['VB','VBD','VBG','VBP','VBZ'] and dependency_results[i][2][1] in ['JJ','JJR','JJS','RB','RBR','RBS']):\n",
    "#                 #if the noun is in subject relation verb\n",
    "#                 #take that verb:\n",
    "#                 myverb=dependency_results[i][0][0]\n",
    "#                 # search for the noun in that sentence\n",
    "                \n",
    "#                 stem_verb=do_stemming(myverb)\n",
    "#                 implicit_aspects.append(stem_verb)\n",
    "\n",
    "#         if(dependency_results[i][1]=='amod'):\n",
    "#             if(dependency_results[i][2][1]=='JJ' and dependency_results[i][0][1] in ['NN','NNS']):\n",
    "#                 implicit_aspects.append(dependency_results[i][2][0])\n",
    "                \n",
    "#         if(dependency_results[i][1]=='nsubjpass'):\n",
    "#             if(dependency_results[i][0][1] in ['JJ','JJR','JJS'] and dependency_results[i][2][1] in ['NN','NNS']):\n",
    "#                 implicit_aspects.append(dependency_results[i][0][0])\n",
    "\n",
    "#     import gensim\n",
    "#     from gensim.models import word2vec\n",
    "\n",
    "#     # get the pretrained word2vec model \n",
    "#     fname=\"/home/ayaqubov/AspectExtractor/myword2vec/word2vecmodel_electronics\"\n",
    "#     #mymodel.save(fname)\n",
    "#     # To get back the model\n",
    "#     mymodel = gensim.models.Word2Vec.load(fname)\n",
    "    \n",
    "#     # remove non-frequent implicit aspects from the sentences --->>> this is needed because we want to compare these to the categories\n",
    "#     unique_implicit_aspects=list(set(implicit_aspects))\n",
    "#     print 'Implicit aspects here:::'\n",
    "#     print unique_implicit_aspects\n",
    "    \n",
    "#     ### mapping of implicit aspect categories\n",
    "#     implicit_aspect_categories=['functionality','weight','price','appearance','behaviour','performance','qualtiy','service','size']\n",
    "#     list2=[]\n",
    "\n",
    "#     for i in range(0,len(unique_implicit_aspects)):\n",
    "#         list2.append(unique_implicit_aspects[i])\n",
    "\n",
    "#     similarities=[]\n",
    "#     for word2 in list2:\n",
    "#         s=[]\n",
    "#         for asp_cat in implicit_aspect_categories:\n",
    "#             try:\n",
    "#                 s.append(mymodel.similarity(word2,asp_cat))\n",
    "#             except:\n",
    "#                 print(\"This word was not found -- improvement with fasttext\")\n",
    "#                 s.append(word2)\n",
    "#         similarities.append(s)\n",
    "\n",
    "#     max_indices=[]\n",
    "#     for i in range(0,len(similarities)):\n",
    "#         m=find_max_index(similarities[i])\n",
    "#         max_indices.append(m)\n",
    "\n",
    "#     # this part does aspect category paring using the results from the WORD2VEC\n",
    "#     aspect_category_pair = dict()\n",
    "#     for i in range(0,len(unique_implicit_aspects)):\n",
    "#         max_l=max_indices[i]\n",
    "#         aspect_category_pair[unique_implicit_aspects[i]]=implicit_aspect_categories[max_l]\n",
    "\n",
    "#     #combining implicit and explicit aspects\n",
    "#     #total_aspects=aspects_explicit\n",
    "#     implicit_categories=aspect_category_pair.values()\n",
    "#     return implicit_categories\n",
    "\n",
    "\n",
    "def combine_aspects(aspects_explicit,implicit_categories):\n",
    "    total_aspects=aspects_explicit\n",
    "    \n",
    "    for ij in range(0,len(implicit_categories)):\n",
    "        total_aspects.append(implicit_categories[ij])\n",
    "    #here are the total aspects\n",
    "    # these aspects represnet the sentence\n",
    "    #print(total_aspects)\n",
    "    #now issue is to find the overall \n",
    "    #return total_aspects\n",
    "    #total_aspects=aspects_explicit\n",
    "    stotal_aspects=set(total_aspects)\n",
    "    return stotal_aspects\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#2 ways to run : either run the main function or run the get_aspects function\n",
    "\n",
    "########################\n",
    "\n",
    "def get_aspects(mysentence):\n",
    "    import os\n",
    "    from nltk.parse import stanford\n",
    "    import os\n",
    "    java_path = \"/usr/lib/jvm/jre-1.8.0\"\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "    myimplicit=extract_implicit(mysentence)\n",
    "    myexplicit=extract_explicit(mysentence)\n",
    "    total_aspects=combine_aspects(myexplicit,myimplicit)\n",
    "    if(total_aspects==[]): # when we dont have specific aspect in the sentence, we add 'GENERAL'\n",
    "        total_aspects.append('GENERAL')\n",
    "    return total_aspects\n",
    "\n",
    "\n",
    "\n",
    "for ifiles in range(0,len(files_in_folder)):\n",
    "    file_directory=input_directory + files_in_folder[ifiles]\n",
    "    print(file_directory)\n",
    "    #file_directory\n",
    "    mycolumnsdf=['REVIEW_WID','REVIEW_FULL_TEXT']\n",
    "    df_aws_reviews=pd.DataFrame(columns=mycolumnsdf)\n",
    "    df_aws_reviews=pd.read_csv(file_directory)\n",
    "    df_aws_reviews['LANGUAGE']=\"\"\n",
    "    num_unique_reviews=df_aws_reviews.shape[0]\n",
    "\n",
    "    ###########################################################\n",
    "    # here comes the check the language part and write correspondingly to the df_aws_reviews dataframe\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    ## data processing\n",
    "    print('Processing...')\n",
    "    index=0\n",
    "    \n",
    "    for u in range(0,num_unique_reviews):\n",
    "        review_id=df_aws_reviews.iloc[u,0]\n",
    "        print(review_id)\n",
    "        review_text=df_aws_reviews.iloc[u,1]\n",
    "        review_str=str(review_text)\n",
    "        #review_strw=review_str.encode('cp1252')## windows encoded\n",
    "        #ureview_str=review_strw.decode('utf-8')\n",
    "        #ureview_str=unicode(review_str,\"utf-8\")\n",
    "        ureview_str=review_str.decode('cp1252').encode('utf-8',errors='ignore')\n",
    "        #print(review_str)\n",
    "        #print(len(review_str))\n",
    "        contain_l=re.search('[a-zA-Z]', review_str)\n",
    "        if(contain_l!='None'):\n",
    "            # handle japanese,arabic,chinese cases because they appear as the ?? marks in the results\n",
    "            try:\n",
    "                text_lang=detect(ureview_str)\n",
    "                #text_lang2=identify_lang(review_str)\n",
    "            except:\n",
    "                print 'Error in reading, keep reading'\n",
    "                #i_debug+=1\n",
    "                continue\n",
    "            #check_words=word_tokenize(review_str)\n",
    "            #for iiii in range(0,len(check_the_words)):\n",
    "            #    if (check_the_words[iiii]=='product' or check_the_words[iiii]=='excellent'  or check_the_words[iiii]=='mouse'):\n",
    "            #        break\n",
    "            #            #continue\n",
    "\n",
    "            if(text_lang=='en'):\n",
    "                df_aws_reviews.iloc[u,2]='English'\n",
    "                index+=1\n",
    "\n",
    "            if(text_lang=='de'):\n",
    "                df_aws_reviews.iloc[u,2]='German'\n",
    "                index+=1\n",
    "\n",
    "            if(text_lang=='fr'):\n",
    "                if('product' in ureview_str or 'excellent' in ureview_str or 'mouse' in ureview_str):\n",
    "                    df_aws_reviews.iloc[u,2]='English'\n",
    "                else:\n",
    "                    df_aws_reviews.iloc[u,2]='French'\n",
    "                index+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # delete reviews that does not have language information\n",
    "    # can also use the drop function from the pandas library\n",
    "    df_aws_reviews=df_aws_reviews[df_aws_reviews['LANGUAGE'] != \"\"]\n",
    "    df_aws_reviews.shape\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # division into the sentences\n",
    "    df_num_of_rows=df_aws_reviews.shape[0]\n",
    "    #mycolumns=['REVIEWSENTENCE_WID','REVIEW_WID','SENTENCE_ID','SKU','COUNTRY','SITE_URL','REVIEW_POSTED_DATE','WORD_COUNT',\n",
    "    #                    'SENTIMENT','STAR_RATING','SENTENCE','PRODUCT_TYPE','PRODUCT_GROUP','PRODUCT_LINE_NAME']\n",
    "    mycolumns_sentence=['textsentence_id','text_id','sentence_id','word_count',\n",
    "                        'Aspects','sentence','language']\n",
    "    df_aws_sentences=pd.DataFrame(columns=mycolumns_sentence)\n",
    "\n",
    "    # adding sentences\n",
    "    index_=0\n",
    "    for i in range(0,df_num_of_rows):\n",
    "        print(\"i is \",i)\n",
    "        this_review=df_aws_reviews.iloc[i,1]\n",
    "            # use try except because errpr was occuring in some cases\n",
    "        try:\n",
    "            sentences_this_review=sent_tokenize(this_review)\n",
    "        except:\n",
    "            print(\"Error in sentence tokenizing\")\n",
    "            continue\n",
    "        num_of_sents=len(sentences_this_review)\n",
    "        current_review_id=str(df_aws_reviews.iloc[i,0])\n",
    "        #print(current_review_id)\n",
    "        if(num_of_sents!=0):\n",
    "            sent_id=0\n",
    "            for j in range(0,num_of_sents):\n",
    "                current_sentence=sentences_this_review[j]\n",
    "                if(current_sentence in [\"!\",\"?\",\".\"]):\n",
    "                    continue\n",
    "                word_count=count_words(current_sentence)\n",
    "                reviewsentence_id=current_review_id+'_'+str(sent_id)#int(current_review_id+'_'+str(sent_id))\n",
    "                # Now calculate the polarity of sentece:\n",
    "                #sentiment_=get_sentiment(current_sentence)\n",
    "                if(df_aws_reviews.iloc[i,2]=='English'):\n",
    "                    aspects_extracted=get_aspects(current_sentence)\n",
    "                else:\n",
    "                    print('Does not support other languages than English...')\n",
    "                ##### since some aspects may be repeated, we do the following:\n",
    "                ### we make the dictionary out of aspects and put that in the cell corresponding to the\n",
    "                ### or we just get the set values:\n",
    "                print(aspects_extracted)\n",
    "                saspects_extracted=list(set(aspects_extracted)) ## this will contain aspects only once.\n",
    "                \n",
    "                one_row=[reviewsentence_id,current_review_id,sent_id,word_count,saspects_extracted,current_sentence,df_aws_reviews.iloc[i,2]]\n",
    "                df_aws_sentences.loc[index_]=one_row\n",
    "                sent_id+=1\n",
    "                index_+=1\n",
    "\n",
    "                \n",
    " \n",
    "    df_aws_sentences[['sentence_id','word_count']]=df_aws_sentences[['sentence_id','word_count']].astype(int)\n",
    "    ########################################################\n",
    "    ## now word frequency table\n",
    "\n",
    "    cols_word_freq= ['reviewsentence_wid','review_wid','sentence_id','word','translated_word','freq']\n",
    "    df_sents_num_of_rows=df_aws_sentences.shape[0]\n",
    "\n",
    "\n",
    "    df_word_freq=pd.DataFrame(columns=cols_word_freq)\n",
    "    # get rid of commas etc\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    import re\n",
    "    def check_num(input_s): \n",
    "        num_format = re.compile(\"^[\\-]?[1-9][0-9]*\\.?[0-9]+$\")\n",
    "        isnumber = re.match(num_format,input_s)\n",
    "        #isnumber=~\n",
    "        if isnumber:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def check_letter(input_s):\n",
    "        #remove len 1 and 2s(come back for len 2 later)\n",
    "        l=len(input_s)\n",
    "        if(l==1 or l==2):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    #stopwords_ = set(stopwords.words('english'))\n",
    "    stopwords_fr=set(stopwords.words('french'))\n",
    "    stopwords_en=set(stopwords.words('english'))\n",
    "    stopwords_ge=set(stopwords.words('german'))\n",
    "\n",
    "\n",
    "    windex_=0\n",
    "\n",
    "    for i in range(0,df_sents_num_of_rows):\n",
    "        #print(i)\n",
    "        sentence=df_aws_sentences.iloc[i,5]\n",
    "        #words=word_tokenize(sentence)\n",
    "        # maybe use try-except block as follows:\n",
    "        #try:\n",
    "        #words=tokenizer.tokenize(sentence)\n",
    "        #except:\n",
    "        #print(\"error in word tokenizing\")\n",
    "        words=tokenizer.tokenize(sentence)\n",
    "        tags_=nltk.pos_tag(words)\n",
    "        num_words=len(words)\n",
    "        for j in range(0,num_words):\n",
    "            word=words[j]\n",
    "            wordlow=word.lower()\n",
    "            # check if it is noun here\n",
    "            translated=wordlow  ## for another language we need translation\n",
    "            freq=1\n",
    "            w_isnum=check_num(wordlow)\n",
    "            one_two_let=check_letter(wordlow)\n",
    "            if(df_aws_sentences.iloc[i,6]=='English'):\n",
    "                stopwords_=stopwords_en\n",
    "            if(df_aws_sentences.iloc[i,6]=='French'):\n",
    "                stopwords_=stopwords_fr\n",
    "            if(df_aws_sentences.iloc[i,6]=='German'):\n",
    "                stopwords_=stopwords_ge\n",
    "            if(wordlow in stopwords_ or w_isnum or one_two_let):\n",
    "                continue\n",
    "\n",
    "\n",
    "            if(tags_[j][1]=='NN' or tags_[j][1]=='NNS' or tags_[j][1]=='NNP'):\n",
    "                # since we expect aspects are more likely to be among the nouns\n",
    "\n",
    "                one_row=[df_aws_sentences.iloc[i,0],df_aws_sentences.iloc[i,1],df_aws_sentences.iloc[i,2],wordlow,translated,freq]\n",
    "                df_word_freq.loc[windex_]=one_row\n",
    "                windex_+=1\n",
    "            #print(windex_)\n",
    "\n",
    "    df_word_freq[['sentence_id','freq']]=df_word_freq[['sentence_id','freq']].astype(int)\n",
    "    df_word_freq.rename(columns={'reviewsentence_wid':'textsentence_id','review_wid':'text_id'},inplace=True)\n",
    "    #######################################################\n",
    "    # give indices a name\n",
    "    #df.index.rename('Index')\n",
    "    \n",
    "    ############################################################################\n",
    "    ## output file generation and moving the Input file from Processed part\n",
    "    ## this basically will be what the users need\n",
    "    sentences_name_to_save=files_in_folder[ifiles][:-4]+'_ouput.csv'\n",
    "    output_sentence_directory=output_directory+sentences_name_to_save\n",
    "    df_aws_sentences.to_csv(output_sentence_directory,index=False)\n",
    "\n",
    "\n",
    "    ###################################\n",
    "    # merge 2 tables  -- in this part\n",
    "    df_merged_output=df_aws_sentences.merge(df_word_freq,how='left',on=['textsentence_id','text_id','sentence_id'])\n",
    "    merged_name_to_save=files_in_folder[ifiles][:-4]+'_output_aspect.csv'\n",
    "    merged_output_directory=output_directory+merged_name_to_save\n",
    "    df_merged_output.to_csv(merged_output_directory,index=False)\n",
    "\n",
    "    # move processed file into folder named \n",
    "    shutil.move(file_directory,processed_directory)\n",
    "\n",
    "\n",
    "    ## 2 output files are generated -- \n",
    "    # 1. sentences with sentiments\n",
    "    # 2. merged table which contains word frequency-- this maybe used for visualisation in Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
